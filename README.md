# Employee-Attrition-
This is 8th July 2025 and I am providing here all the information about the work I have done till now under DSKC Summer Internship that I joined on 8th June,2025. 
When I joined this internship, we searched for areas where we could solve a potential problem using a machine learning model - the most optimised, the most hypertuned..
After discussions, deliberations and constant meetings with experienced professors at Miranda House College, we came up with the idea of working on detecting potential employee attrition in companies and after surfing through the research papers, we realised that this was actually a hot topic of the CS AI ML research field..with the most recent research paper published on 31st May,2025 itself..
It was as if destiny too wanted a serious contribution in this field from us and after thorough review of already published research papers, we found out that there was a synthetic dataset developed by IBM for providing a common ground for all researchers to implement their models on and compare results. We too hence picked up this dataset but realised it was highly imbalanced with 1:5 ratio of Attrition and No Attrition Class. Since our target was to catch most of the attrition cases as  companies want to retain their valuable employees and a high recall for the attrition class in this case seemed to us the most useful to focus on as a result as even if we could be flagging some of the employees who are actually not going to leave as the case of Attrition but we won't miss any real case so this trade off between recall and precision was done and a balanced F1 score was our target. From the beginning, trees seemed the most useful and intuitive algorithm for this highly engineered synthetic dataset by IBM and after multiple trials, we came up with the Stacking Model which had Random Forest, XGBoost, LGBM as base learners and ETC as meta learner. again each one of them has trees lying at its core only built differently. This unique combination came to us after thorough analyis. Not only this, we tried multiple combination of parameters and applied parameter tuning mindfully for each of the learners for the best recall of the attrition class. We also applied threhshold tuning for giving more weightage to the minority class. I have attached here the Jupyter Notebook containing the code for the Stacking Model that is completely ours, the four weekly reports that we had to submit to our supervisors and the dataset that we have used. The weekly reports show how gradually we proceeded ahead and finalised our final model. We have also applied this model on a real survey based dataset by Watson healthcare and this model gave its best performance on that dataset, proving its usecase on a general level. After completing one month of this internship and contributing such an innovation in ML world, I am feeling deep satisfaction and I am happy to share my work here.
Thankyou.
